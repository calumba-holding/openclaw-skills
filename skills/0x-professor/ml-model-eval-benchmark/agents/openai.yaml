interface:
  display_name: "ML Model Eval Benchmark"
  short_description: "Benchmark model metrics with clear ranking"
  default_prompt: "Use $ml-model-eval-benchmark to rank model candidates by weighted metrics."
