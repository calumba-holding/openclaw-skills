#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å“ç‰Œç›‘æ§æœç´¢çˆ¬è™« - SerpAPI ç‰ˆæœ¬
ä½¿ç”¨ SerpAPI è¿›è¡Œå¯é çš„æœç´¢ï¼Œæ”¯æŒ Googleã€ç™¾åº¦ç­‰å¤šä¸ªæœç´¢å¼•æ“
"""

import json
import sys
import os
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import requests
from urllib.parse import quote

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸º UTF-8
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')


class SerpAPISearcher:
    """SerpAPI æœç´¢å™¨"""
    
    def __init__(self, api_key: Optional[str] = None, engine: str = 'google'):
        """
        åˆå§‹åŒ– SerpAPI æœç´¢å™¨
        
        Args:
            api_key: SerpAPI API Keyï¼ˆå¦‚æœä¸æä¾›ï¼Œä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            engine: æœç´¢å¼•æ“ï¼ˆbaidu, bingï¼Œ googleï¼‰
        """
        self.api_key = api_key or os.environ.get('SERPAPI_KEY')
        if not self.api_key:
            raise ValueError("éœ€è¦æä¾› SERPAPI_KEY ç¯å¢ƒå˜é‡æˆ– api_key å‚æ•°")
        
        self.engine = engine
        self.base_url = "https://serpapi.com/search"
        
    def _detect_platform(self, url: str) -> str:
        """æ ¹æ®URLæ£€æµ‹å¹³å°"""
        url_lower = url.lower()
        
        if 'weibo.com' in url_lower or 'weibo.cn' in url_lower:
            return 'weibo'
        elif 'xiaohongshu.com' in url_lower or 'xhslink.com' in url_lower:
            return 'xiaohongshu'
        elif 'zhihu.com' in url_lower:
            return 'zhihu'
        elif 'autohome.com' in url_lower:
            return 'autohome'
        elif 'dongchedi.com' in url_lower:
            return 'dongchedi'
        elif 'yiche.com' in url_lower or 'bitauto.com' in url_lower:
            return 'yiche'
        elif 'tieba.baidu.com' in url_lower:
            return 'tieba'
        elif 'douyin.com' in url_lower:
            return 'douyin'
        elif 'kuaishou.com' in url_lower:
            return 'kuaishou'
        else:
            return 'other'
    
    def search(self, query: str, num_results: int = 20, **kwargs) -> List[Dict]:
        """
        æ‰§è¡Œæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            num_results: ç»“æœæ•°é‡
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆå¦‚ location, hl ç­‰ï¼‰
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        params = {
            'api_key': self.api_key,
            'engine': self.engine,
            'q': query,
            'num': num_results,
        }
        
        # æ·»åŠ é¢å¤–å‚æ•°
        params.update(kwargs)
        
        # å¦‚æœæ˜¯ç™¾åº¦æœç´¢ï¼Œä½¿ç”¨ä¸­æ–‡
        if self.engine == 'baidu':
            params['hl'] = 'zh-cn'
        
        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            # è°ƒè¯•ï¼šæ‰“å°åŸå§‹å“åº”ï¼ˆä»…åœ¨è°ƒè¯•æ¨¡å¼ä¸‹ï¼‰
            if os.environ.get('DEBUG'):
                print(f"\n=== åŸå§‹ SerpAPI å“åº” ===", file=sys.stderr)
                print(json.dumps(data, ensure_ascii=False, indent=2)[:2000], file=sys.stderr)
                print("=" * 50, file=sys.stderr)
            
            return self._parse_results(data)
            
        except requests.exceptions.RequestException as e:
            print(f"SerpAPI è¯·æ±‚å¤±è´¥: {e}", file=sys.stderr)
            return []
        except Exception as e:
            print(f"è§£æç»“æœå¤±è´¥: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc(file=sys.stderr)
            return []
    
    def _extract_numbers_from_text(self, text: str) -> Dict[str, int]:
        """ä»æ–‡æœ¬ä¸­æå–æ•°å­—ä¿¡æ¯ï¼ˆç‚¹èµã€è¯„è®ºã€è½¬å‘ç­‰ï¼‰"""
        import re
        
        numbers = {
            'likes': 0,
            'comments': 0,
            'shares': 0,
            'followers': 0
        }
        
        # åŒ¹é…å„ç§æ•°å­—æ ¼å¼
        # ä¾‹å¦‚: "58601 31381 Ã±626294" (æ”¶è— è½¬å‘ è¯„è®º)
        # "744207ç²‰ä¸" "74.4ä¸‡ç²‰ä¸"
        
        # ç²‰ä¸æ•°
        followers_match = re.search(r'(\d+(?:\.\d+)?)[ä¸‡åƒ]?ç²‰ä¸', text)
        if followers_match:
            num_str = followers_match.group(1)
            if 'ä¸‡' in text[followers_match.start():followers_match.end()]:
                numbers['followers'] = int(float(num_str) * 10000)
            elif 'åƒ' in text[followers_match.start():followers_match.end()]:
                numbers['followers'] = int(float(num_str) * 1000)
            else:
                numbers['followers'] = int(num_str)
        
        # ç‚¹èµ/æ”¶è— (é€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªæ•°å­—)
        likes_match = re.search(r'Ã»æ”¶è—\s*(\d+)', text)
        if likes_match:
            numbers['likes'] = int(likes_match.group(1))
        
        # è½¬å‘ (é€šå¸¸æ˜¯ç¬¬äºŒä¸ªæ•°å­—)
        shares_match = re.search(r'(\d+)\s+(\d+)\s+Ã±(\d+)', text)
        if shares_match:
            numbers['shares'] = int(shares_match.group(2))
            numbers['comments'] = int(shares_match.group(3))
        
        return numbers
    
    def _extract_publish_time(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–å‘å¸ƒæ—¶é—´"""
        import re
        
        # åŒ¹é…æ—¥æœŸæ ¼å¼: "2æœˆ18æ—¥ 23:52", "2025-9-26 10:30"
        time_patterns = [
            r'(\d{1,2}æœˆ\d{1,2}æ—¥\s+\d{1,2}:\d{2})',
            r'(\d{4}-\d{1,2}-\d{1,2}\s+\d{1,2}:\d{2})',
            r'(\d{1,2}å°æ—¶å‰)',
            r'(\d{1,2}åˆ†é’Ÿå‰)',
            r'(æ˜¨å¤©\s+\d{1,2}:\d{2})',
        ]
        
        for pattern in time_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        
        return ''
    
    def _extract_author(self, text: str, url: str) -> tuple:
        """ä»æ–‡æœ¬å’ŒURLä¸­æå–ä½œè€…ä¿¡æ¯"""
        import re
        
        author = ''
        author_id = ''
        verified = False
        
        # ä»æ ‡é¢˜æˆ–å†…å®¹ä¸­æå–ä½œè€…
        # ä¾‹å¦‚: "ç†æƒ³æ±½è½¦çš„å¾®åš", "@ç†æƒ³æ±½è½¦", "ææƒ³çš„å¾®åš"
        author_match = re.search(r'(@?[\u4e00-\u9fa5a-zA-Z0-9_-]+)(?:çš„å¾®åš|æˆä¸º)', text)
        if author_match:
            author = author_match.group(1).replace('@', '')
        
        # ä»URLä¸­æå–ç”¨æˆ·ID
        # ä¾‹å¦‚: weibo.com/6001272153, weibo.com/lixiangzhizao
        url_match = re.search(r'weibo\.com/(?:u/)?([a-zA-Z0-9_]+)', url)
        if url_match:
            author_id = url_match.group(1)
        
        # æ£€æµ‹è®¤è¯æ ‡è¯†
        if 'å®˜æ–¹' in text or 'è®¤è¯' in text or 'CEO' in text:
            verified = True
        
        return author, author_id, verified
    
    def _parse_results(self, data: Dict) -> List[Dict]:
        """è§£ææœç´¢ç»“æœ"""
        results = []
        
        # è·å–æœ‰æœºæœç´¢ç»“æœ
        organic_results = data.get('organic_results', [])
        
        for item in organic_results:
            url = item.get('link', '')
            platform = self._detect_platform(url)
            
            title = item.get('title', '')
            snippet = item.get('snippet', '')
            full_text = f"{title} {snippet}"
            
            # æå–æ•°å­—ä¿¡æ¯
            numbers = self._extract_numbers_from_text(full_text)
            
            # æå–å‘å¸ƒæ—¶é—´
            publish_time = self._extract_publish_time(full_text) or item.get('date', '')
            
            # æå–ä½œè€…ä¿¡æ¯
            author, author_id, verified = self._extract_author(full_text, url)
            
            result = {
                'platform': platform,
                'title': title,
                'content': snippet,
                'url': url,
                'source': f'serpapi_{self.engine}',
                'publish_time': publish_time,
                'author': author,
                'author_id': author_id,
                'followers': numbers['followers'],
                'verified': verified,
                'likes': numbers['likes'],
                'comments': numbers['comments'],
                'shares': numbers['shares'],
            }
            results.append(result)
        
        return results


class PlatformSearcher:
    """å¤šå¹³å°æœç´¢ç®¡ç†å™¨"""
    
    def __init__(self, api_key: Optional[str] = None, engine: str = 'google', use_mock: bool = False, 
                 exclude_official: bool = True, brand_keywords: List[str] = None):
        """
        åˆå§‹åŒ–æœç´¢å™¨
        
        Args:
            api_key: SerpAPI API Key
            engine: æœç´¢å¼•æ“ï¼ˆgoogle, baidu, bingï¼‰
            use_mock: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            exclude_official: æ˜¯å¦æ’é™¤å“ç‰Œå®˜æ–¹è´¦å·
            brand_keywords: å“ç‰Œå…³é”®è¯åˆ—è¡¨ï¼Œç”¨äºè¯†åˆ«å®˜æ–¹è´¦å·
        """
        self.use_mock = use_mock
        self.exclude_official = exclude_official
        self.brand_keywords = brand_keywords or []
        
        if not use_mock:
            self.searcher = SerpAPISearcher(api_key, engine)
        
        self.platform_sites = {
            'weibo': 'site:weibo.com OR site:weibo.cn',
            'xiaohongshu': 'site:xiaohongshu.com',
            'zhihu': 'site:zhihu.com',
            'autohome': 'site:autohome.com.cn',
            'dongchedi': 'site:dongchedi.com',
            'yiche': 'site:yiche.com OR site:bitauto.com',
            'tieba': 'site:tieba.baidu.com',
            'douyin': 'site:douyin.com',
            'kuaishou': 'site:kuaishou.com',
        }
        
        self.platform_names = {
            'weibo': 'å¾®åš',
            'xiaohongshu': 'å°çº¢ä¹¦',
            'zhihu': 'çŸ¥ä¹',
            'autohome': 'æ±½è½¦ä¹‹å®¶',
            'dongchedi': 'æ‡‚è½¦å¸',
            'yiche': 'æ˜“è½¦ç½‘',
            'tieba': 'ç™¾åº¦è´´å§',
            'douyin': 'æŠ–éŸ³',
            'kuaishou': 'å¿«æ‰‹',
        }
        
        # å¸¸è§å®˜æ–¹è´¦å·å…³é”®è¯
        self.official_keywords = [
            'å®˜æ–¹', 'æ³•åŠ¡', 'å®¢æœ', 'æœåŠ¡', 'official',
            'å…¬å¸', 'é›†å›¢', 'æ€»éƒ¨', 'å“ç‰Œ'
        ]
    
    def _is_official_account(self, author: str, url: str, content: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºå“ç‰Œå®˜æ–¹è´¦å·
        
        Args:
            author: ä½œè€…åç§°
            url: å†…å®¹URL
            content: å†…å®¹æ–‡æœ¬
        
        Returns:
            True å¦‚æœæ˜¯å®˜æ–¹è´¦å·
        """
        # æ£€æŸ¥ä½œè€…åç§°
        author_lower = author.lower()
        
        for keyword in self.brand_keywords:
            keyword_lower = keyword.lower()
            
            # å¦‚æœä½œè€…ååŒ…å«å“ç‰Œå
            if keyword_lower in author_lower or keyword in author:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å®˜æ–¹å…³é”®è¯
                for official_kw in self.official_keywords:
                    if official_kw in author or official_kw in author_lower:
                        return True
                
                # å¦‚æœä½œè€…åå°±æ˜¯å“ç‰Œåï¼ˆå®Œå…¨åŒ¹é…æˆ–é«˜åº¦ç›¸ä¼¼ï¼‰
                if author == keyword or author.replace(' ', '').replace('æ±½è½¦', '') == keyword.replace(' ', '').replace('æ±½è½¦', ''):
                    return True
        
        # æ£€æŸ¥URLï¼ˆå®˜æ–¹åŸŸåï¼‰
        for keyword in self.brand_keywords:
            keyword_pinyin = self._to_pinyin(keyword)
            if keyword_pinyin and keyword_pinyin in url.lower():
                # æ£€æŸ¥æ˜¯å¦æ˜¯å®˜æ–¹åŸŸåï¼ˆä¸æ˜¯ç¤¾äº¤åª’ä½“å¹³å°ï¼‰
                if keyword_pinyin + '.com' in url or keyword_pinyin + '.cn' in url:
                    return True
        
        return False
    
    def _to_pinyin(self, text: str) -> str:
        """ç®€å•çš„å“ç‰Œåè½¬æ‹¼éŸ³ï¼ˆç”¨äºURLåŒ¹é…ï¼‰"""
        # å¸¸è§å“ç‰Œæ‹¼éŸ³æ˜ å°„
        pinyin_map = {
            'ç†æƒ³æ±½è½¦': 'lixiang',
            'ç†æƒ³': 'lixiang',
            'è”šæ¥': 'nio',
            'å°é¹': 'xiaopeng',
            'é—®ç•Œ': 'aito',
            'æ¯”äºšè¿ª': 'byd',
        }
        return pinyin_map.get(text, '')
    
    def _generate_mock_data(self, keyword: str, platform: str, max_results: int) -> List[Dict]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"""
        results = []
        platform_name = self.platform_names.get(platform, platform)
        
        # ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨¡æ‹Ÿæ•°æ®ï¼ŒåŒ…æ‹¬å®˜æ–¹å’Œéå®˜æ–¹
        mock_authors = [
            {'name': f'{platform_name}ç”¨æˆ·{i+1}', 'is_official': False, 'followers': 5000 + i * 1000},
            {'name': f'æ±½è½¦è¯„æµ‹å¸ˆ{i+1}', 'is_official': False, 'followers': 50000 + i * 10000},
            {'name': f'{keyword}', 'is_official': True, 'followers': 700000},  # å®˜æ–¹è´¦å·
        ]
        
        for i in range(min(3, max_results)):
            author_info = mock_authors[i % len(mock_authors)]
            
            # å¦‚æœæ’é™¤å®˜æ–¹è´¦å·ï¼Œè·³è¿‡å®˜æ–¹æ•°æ®
            if self.exclude_official and author_info['is_official']:
                continue
            
            result = {
                'platform': platform,
                'title': f'{platform_name}ä¸Šå…³äº{keyword}çš„è®¨è®º #{i+1}',
                'content': f'è¿™æ˜¯ä¸€æ¡æ¥è‡ª{platform_name}çš„å…³äº{keyword}çš„æ¨¡æ‹Ÿå†…å®¹ã€‚åŒ…å«ç”¨æˆ·çš„çœŸå®ä½“éªŒå’Œè¯„ä»·ã€‚',
                'author': author_info['name'],
                'author_id': f'{platform}_user_{i+1}',
                'followers': author_info['followers'],
                'verified': author_info['is_official'],
                'url': f'https://example.com/{platform}/post/{i+1}',
                'publish_time': (datetime.now() - timedelta(hours=i*2)).isoformat(),
                'likes': 100 * (i + 1),
                'comments': 20 * (i + 1),
                'shares': 10 * (i + 1),
                'is_official': author_info['is_official'],
            }
            results.append(result)
        
        return results
    
    def search_platform(self, keyword: str, platform: str, max_results: int = 20, hours: int = 24) -> List[Dict]:
        """
        åœ¨æŒ‡å®šå¹³å°æœç´¢
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            platform: å¹³å°åç§°
            max_results: æœ€å¤§ç»“æœæ•°
            hours: æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if self.use_mock:
            return self._generate_mock_data(keyword, platform, max_results)
        
        # æ„å»ºæœç´¢æŸ¥è¯¢ - ä¸ä½¿ç”¨ site: è¿‡æ»¤å™¨ï¼Œè€Œæ˜¯åœ¨å…³é”®è¯ä¸­åŒ…å«å¹³å°åç§°
        platform_name = self.platform_names.get(platform, platform)
        
        # å¯¹äºä¸­æ–‡å¹³å°ï¼Œç›´æ¥æœç´¢ "å…³é”®è¯ + å¹³å°åç§°"ï¼Œè®©æœç´¢å¼•æ“è‡ªç„¶åŒ¹é…
        query = f'{keyword} {platform_name}'
        
        # æ·»åŠ æ—¶é—´è¿‡æ»¤ï¼ˆå¦‚æœæ”¯æŒï¼‰
        search_params = {}
        if hours <= 24:
            # Google/Baidu æ”¯æŒæ—¶é—´è¿‡æ»¤
            search_params['tbs'] = f'qdr:d'  # æœ€è¿‘ä¸€å¤©
        elif hours <= 168:  # 7å¤©
            search_params['tbs'] = f'qdr:w'  # æœ€è¿‘ä¸€å‘¨
        
        # æ‰§è¡Œæœç´¢
        results = self.searcher.search(query, max_results, **search_params)
        
        # è¿‡æ»¤ç»“æœ
        filtered_results = []
        official_count = 0
        
        for r in results:
            detected_platform = self.searcher._detect_platform(r['url'])
            
            # åªä¿ç•™ç›®æ ‡å¹³å°
            if platform != 'all' and detected_platform != platform:
                continue
            
            r['platform'] = platform  # ç¡®ä¿å¹³å°æ ‡è®°æ­£ç¡®
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå®˜æ–¹è´¦å·
            is_official = self._is_official_account(r['author'], r['url'], r['content'])
            r['is_official'] = is_official
            
            if is_official:
                official_count += 1
                if self.exclude_official:
                    continue  # æ’é™¤å®˜æ–¹è´¦å·
            
            filtered_results.append(r)
        
        # è¾“å‡ºè¿‡æ»¤ç»Ÿè®¡
        if official_count > 0:
            print(f"  â„¹ï¸  è¿‡æ»¤äº† {official_count} æ¡å®˜æ–¹è´¦å·å†…å®¹", file=sys.stderr)
        
        return filtered_results
    
    def search_all(self, keyword: str, platforms: List[str], max_results_per_platform: int = 20, hours: int = 24) -> Dict[str, List[Dict]]:
        """
        åœ¨æ‰€æœ‰æŒ‡å®šå¹³å°æœç´¢
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            platforms: å¹³å°åˆ—è¡¨
            max_results_per_platform: æ¯ä¸ªå¹³å°çš„æœ€å¤§ç»“æœæ•°
            hours: æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
        
        Returns:
            æŒ‰å¹³å°åˆ†ç»„çš„æœç´¢ç»“æœ
        """
        results = {}
        
        for platform in platforms:
            print(f"æ­£åœ¨æœç´¢ {platform}...", file=sys.stderr)
            
            try:
                platform_results = self.search_platform(keyword, platform, max_results_per_platform, hours)
                results[platform] = platform_results
                print(f"{platform} æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(platform_results)} æ¡ç»“æœ", file=sys.stderr)
                
                # é¿å…è¯·æ±‚è¿‡å¿«ï¼ˆSerpAPI æœ‰é€Ÿç‡é™åˆ¶ï¼‰
                if not self.use_mock and platform != platforms[-1]:
                    time.sleep(1)
                    
            except Exception as e:
                print(f"{platform} æœç´¢å‡ºé”™: {e}", file=sys.stderr)
                import traceback
                traceback.print_exc(file=sys.stderr)
                results[platform] = []
        
        return results


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python search_crawler_serpapi.py <å…³é”®è¯> [å¹³å°åˆ—è¡¨] [æ¯å¹³å°æœ€å¤§ç»“æœæ•°] [æ—¶é—´èŒƒå›´(å°æ—¶)] [--mock] [--include-official]")
        print("")
        print("ç¤ºä¾‹:")
        print("  # ä½¿ç”¨ SerpAPI æœç´¢ï¼ˆé»˜è®¤æ’é™¤å®˜æ–¹è´¦å·ï¼‰")
        print("  python search_crawler_serpapi.py 'ç†æƒ³æ±½è½¦' 'weibo,xiaohongshu,zhihu' 10 24")
        print("")
        print("  # åŒ…å«å®˜æ–¹è´¦å·")
        print("  python search_crawler_serpapi.py 'ç†æƒ³æ±½è½¦' 'weibo,zhihu' 10 24 --include-official")
        print("")
        print("  # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        print("  python search_crawler_serpapi.py 'ç†æƒ³æ±½è½¦' 'weibo,zhihu' 10 24 --mock")
        print("")
        print("æ”¯æŒçš„å¹³å°:")
        print("  - weibo: å¾®åš")
        print("  - xiaohongshu: å°çº¢ä¹¦")
        print("  - zhihu: çŸ¥ä¹")
        print("  - autohome: æ±½è½¦ä¹‹å®¶")
        print("  - dongchedi: æ‡‚è½¦å¸")
        print("  - yiche: æ˜“è½¦ç½‘")
        print("  - tieba: ç™¾åº¦è´´å§")
        print("  - douyin: æŠ–éŸ³")
        print("  - kuaishou: å¿«æ‰‹")
        print("")
        print("ç¯å¢ƒå˜é‡:")
        print("  SERPAPI_KEY: SerpAPI API Keyï¼ˆå¿…éœ€ï¼Œé™¤éä½¿ç”¨ --mockï¼‰")
        print("  SERPAPI_ENGINE: æœç´¢å¼•æ“ï¼ˆå¯é€‰ï¼Œé»˜è®¤: baiduï¼Œå¯é€‰: google, bingï¼‰")
        print("")
        print("é€‰é¡¹:")
        print("  --mock: ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œä¸æ¶ˆè€— API é…é¢")
        print("  --include-official: åŒ…å«å“ç‰Œå®˜æ–¹è´¦å·ï¼ˆé»˜è®¤æ’é™¤ï¼‰")
        print("")
        print("èˆ†æƒ…ç›‘æ§è¯´æ˜:")
        print("  é»˜è®¤æ’é™¤å“ç‰Œå®˜æ–¹è´¦å·ï¼Œåªå…³æ³¨ç¬¬ä¸‰æ–¹è‡ªåª’ä½“å’Œç”¨æˆ·çš„çœŸå®å£°éŸ³")
        print("  å®˜æ–¹è´¦å·è¯†åˆ«è§„åˆ™ï¼šä½œè€…ååŒ…å«å“ç‰Œå + 'å®˜æ–¹'/'æ³•åŠ¡'/'å®¢æœ'ç­‰å…³é”®è¯")
        print("")
        print("è·å– API Key:")
        print("  è®¿é—® https://serpapi.com/ æ³¨å†Œå¹¶è·å–å…è´¹ API Key")
        print("  å…è´¹é¢åº¦: 100 æ¬¡æœç´¢/æœˆ")
        sys.exit(1)
    
    keyword = sys.argv[1]
    platforms = sys.argv[2].split(',') if len(sys.argv) > 2 else ['weibo', 'xiaohongshu', 'zhihu']
    max_results = int(sys.argv[3]) if len(sys.argv) > 3 and sys.argv[3] not in ['--mock', '--include-official'] else 10
    hours = int(sys.argv[4]) if len(sys.argv) > 4 and sys.argv[4] not in ['--mock', '--include-official'] else 24
    use_mock = '--mock' in sys.argv
    include_official = '--include-official' in sys.argv
    exclude_official = not include_official
    
    # è¯»å–é…ç½®
    api_key = os.environ.get('SERPAPI_KEY')
    engine = os.environ.get('SERPAPI_ENGINE', 'baidu')
    
    # æ£€æŸ¥ API Key
    if not use_mock and not api_key:
        print("âŒ é”™è¯¯: æœªè®¾ç½® SERPAPI_KEY ç¯å¢ƒå˜é‡", file=sys.stderr)
        print("", file=sys.stderr)
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡:", file=sys.stderr)
        print("  export SERPAPI_KEY='your_api_key'", file=sys.stderr)
        print("", file=sys.stderr)
        print("æˆ–ä½¿ç”¨ --mock å‚æ•°è¿›è¡Œæµ‹è¯•:", file=sys.stderr)
        print(f"  python search_crawler_serpapi.py '{keyword}' '{','.join(platforms)}' {max_results} {hours} --mock", file=sys.stderr)
        sys.exit(1)
    
    # æ˜¾ç¤ºé…ç½®
    if use_mock:
        print("âš ï¸  ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼", file=sys.stderr)
    else:
        print(f"âœ“ ä½¿ç”¨ SerpAPI ({engine})", file=sys.stderr)
    
    print(f"æœç´¢å…³é”®è¯: {keyword}", file=sys.stderr)
    print(f"æœç´¢å¹³å°: {', '.join(platforms)}", file=sys.stderr)
    print(f"æ¯å¹³å°æœ€å¤§ç»“æœ: {max_results}", file=sys.stderr)
    print(f"æ—¶é—´èŒƒå›´: {hours} å°æ—¶", file=sys.stderr)
    print(f"å®˜æ–¹è´¦å·: {'åŒ…å«' if include_official else 'æ’é™¤'}", file=sys.stderr)
    print("", file=sys.stderr)
    
    # åˆ›å»ºæœç´¢å™¨
    try:
        # ä»å…³é”®è¯ä¸­æå–å“ç‰Œåï¼ˆç”¨äºè¯†åˆ«å®˜æ–¹è´¦å·ï¼‰
        brand_keywords = [keyword]
        
        searcher = PlatformSearcher(
            api_key=api_key, 
            engine=engine, 
            use_mock=use_mock,
            exclude_official=exclude_official,
            brand_keywords=brand_keywords
        )
        results = searcher.search_all(keyword, platforms, max_results, hours)
        
        # ç»Ÿè®¡æ€»ç»“æœæ•°
        total_results = sum(len(r) for r in results.values())
        print(f"\nâœ“ æœç´¢å®Œæˆï¼å…±æ‰¾åˆ° {total_results} æ¡ç»“æœ", file=sys.stderr)
        
        # æ˜¾ç¤ºé…é¢ä¿¡æ¯ï¼ˆå¦‚æœä¸æ˜¯ mock æ¨¡å¼ï¼‰
        if not use_mock:
            print(f"", file=sys.stderr)
            print(f"ğŸ’¡ æç¤º: æœ¬æ¬¡æœç´¢æ¶ˆè€—äº† {len(platforms)} æ¬¡ API è°ƒç”¨", file=sys.stderr)
            print(f"   SerpAPI å…è´¹é¢åº¦: 100 æ¬¡/æœˆ", file=sys.stderr)
        
        # è¾“å‡ºJSONç»“æœ
        print(json.dumps(results, ensure_ascii=False, indent=2))
        
    except ValueError as e:
        print(f"âŒ é…ç½®é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        sys.exit(1)


if __name__ == '__main__':
    main()
