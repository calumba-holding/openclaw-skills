#!/usr/bin/env python3
"""
å°çº¢ä¹¦å†…å®¹è¯¦æƒ…æŠ“å–
åŸºäºå·²è·å–çš„åˆ—è¡¨ï¼Œæ‰¹é‡æŠ“å–è¯¦æƒ…æ­£æ–‡
"""

import json
import requests
import time

class XHSMCPClient:
    def __init__(self, base_url="http://localhost:18060"):
        self.base_url = base_url
        self.session = requests.Session()
    
    def get_feed_detail(self, feed_id, xsec_token):
        """è·å–å¸–å­è¯¦æƒ…"""
        data = {
            "feed_id": feed_id,
            "xsec_token": xsec_token,
            "load_all_comments": False
        }
        
        resp = self.session.post(
            f"{self.base_url}/api/v1/feeds/detail",
            json=data,
            timeout=60
        )
        return resp.json()

def fetch_details():
    """æŠ“å–æ‰€æœ‰å†…å®¹çš„è¯¦æƒ…"""
    
    # è¯»å–å·²æŠ“å–çš„ç»“æœ
    with open('/tmp/xhs_ai_crawled.json', 'r') as f:
        crawl_data = json.load(f)
    
    client = XHSMCPClient()
    
    print("ğŸ“ å¼€å§‹æŠ“å–æ­£æ–‡å†…å®¹...\n")
    
    detailed_notes = []
    
    for i, note in enumerate(crawl_data['notes'], 1):
        print(f"[{i}/10] æŠ“å–: {note['title'][:30]}...", end=" ", flush=True)
        
        try:
            result = client.get_feed_detail(note['id'], note.get('xsec_token', ''))
            
            if result.get('success'):
                feed_data = result.get('data', {}).get('feed', {})
                note_card = feed_data.get('noteCard', {})
                
                # æå–æ­£æ–‡
                desc = note_card.get('desc', '')
                title = note_card.get('displayTitle', note['title'])
                
                detailed_note = {
                    **note,
                    'full_title': title,
                    'description': desc,
                    'images': [img.get('urlDefault', '') for img in note_card.get('imageList', [])[:5]],
                    'tags': note_card.get('tagList', []),
                    'fetch_success': True
                }
                
                detailed_notes.append(detailed_note)
                print(f"âœ… æ­£æ–‡ {len(desc)} å­—")
            else:
                print(f"âŒ {result.get('message', 'å¤±è´¥')}")
                detailed_notes.append({**note, 'fetch_success': False})
            
            # é—´éš” 2 ç§’ï¼Œé¿å…è§¦å‘é£æ§
            time.sleep(2)
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            detailed_notes.append({**note, 'fetch_success': False, 'error': str(e)})
    
    # ä¿å­˜å®Œæ•´æ•°æ®
    output = {
        **crawl_data,
        'notes': detailed_notes,
        'detail_fetched_at': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open('/tmp/xhs_ai_detailed.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    # ç»Ÿè®¡
    success_count = sum(1 for n in detailed_notes if n.get('fetch_success'))
    print(f"\n{'='*50}")
    print(f"âœ… æŠ“å–å®Œæˆ: {success_count}/{len(detailed_notes)} æ¡æˆåŠŸ")
    print(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ° /tmp/xhs_ai_detailed.json")
    print(f"{'='*50}")
    
    # æ˜¾ç¤ºç¬¬ä¸€æ¡æ­£æ–‡é¢„è§ˆ
    for note in detailed_notes[:2]:
        if note.get('fetch_success'):
            print(f"\nã€{note['title'][:30]}...ã€‘")
            print(f"æ­£æ–‡é¢„è§ˆ: {note.get('description', '')[:200]}...")
    
    return output

if __name__ == "__main__":
    fetch_details()
