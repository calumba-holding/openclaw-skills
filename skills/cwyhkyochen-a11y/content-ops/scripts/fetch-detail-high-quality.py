#!/usr/bin/env python3
"""
å°çº¢ä¹¦ä¼˜è´¨å†…å®¹è¯¦æƒ…æŠ“å–
åŸºäºåˆ—è¡¨ç»“æœï¼Œç­›é€‰é«˜è´¨é‡å†…å®¹ï¼ŒæŠ“å–è¯¦æƒ…å’Œè¯„è®º
"""

import json
import requests
import time
import os

class XHSMCPClient:
    def __init__(self, base_url="http://localhost:18060"):
        self.base_url = base_url
        self.session = requests.Session()
    
    def get_feed_detail(self, feed_id, xsec_token):
        """è·å–å¸–å­è¯¦æƒ…ï¼ˆåŒ…å«å®Œæ•´æ­£æ–‡å’Œè¯„è®ºï¼‰"""
        data = {
            "feed_id": feed_id,
            "xsec_token": xsec_token,
            "load_all_comments": True,  # åŠ è½½è¯„è®º
            "limit": 20  # å‰20æ¡è¯„è®º
        }
        
        resp = self.session.post(
            f"{self.base_url}/api/v1/feeds/detail",
            json=data,
            timeout=60
        )
        return resp.json()

def fetch_high_quality_details():
    """æŠ“å–é«˜è´¨é‡å†…å®¹çš„è¯¦æƒ…"""
    
    # è¯»å–åˆ—è¡¨æ•°æ®
    with open('/tmp/xhs_ai_crawled.json', 'r') as f:
        crawl_data = json.load(f)
    
    # ç­›é€‰é«˜è´¨é‡å†…å®¹ï¼ˆè´¨é‡åˆ† >= 8ï¼‰
    high_quality = [n for n in crawl_data['notes'] if n.get('quality_score', 0) >= 8]
    
    print(f"ğŸ¯ ç­›é€‰å‡º {len(high_quality)} æ¡é«˜è´¨é‡å†…å®¹")
    print("=" * 70)
    
    client = XHSMCPClient()
    
    # åˆ›å»ºæœ¬åœ°å­˜å‚¨ç›®å½•
    corpus_dir = os.path.expanduser('~/.openclaw/workspace/content-ops-workspace/corpus/raw')
    os.makedirs(corpus_dir, exist_ok=True)
    
    detailed_contents = []
    
    for i, note in enumerate(high_quality, 1):
        print(f"\n[{i}/{len(high_quality)}] æŠ“å–è¯¦æƒ…: {note['title'][:40]}...")
        
        try:
            result = client.get_feed_detail(note['id'], note.get('xsec_token', ''))
            
            if result.get('success'):
                feed_data = result.get('data', {}).get('feed', {})
                note_card = feed_data.get('noteCard', {})
                
                # æå–å®Œæ•´æ•°æ®
                full_content = {
                    'note_id': note['id'],
                    'title': note_card.get('displayTitle', note['title']),
                    'description': note_card.get('desc', ''),
                    'type': note_card.get('type', 'unknown'),
                    'author': {
                        'nickname': note_card.get('user', {}).get('nickname', ''),
                        'user_id': note_card.get('user', {}).get('userId', ''),
                    },
                    'interaction': note_card.get('interactInfo', {}),
                    'images': [img.get('urlDefault', '') for img in note_card.get('imageList', [])],
                    'tags': note_card.get('tagList', []),
                    'comments': feed_data.get('comments', []),
                    'fetched_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'source_url': f"https://www.xiaohongshu.com/explore/{note['id']}"
                }
                
                # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
                filename = f"xhs_{note['id']}_{int(time.time())}.json"
                filepath = os.path.join(corpus_dir, filename)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(full_content, f, ensure_ascii=False, indent=2)
                
                detailed_contents.append(full_content)
                
                desc_len = len(full_content['description'])
                comment_count = len(full_content['comments'])
                img_count = len(full_content['images'])
                
                print(f"    âœ… æˆåŠŸ")
                print(f"    ğŸ“ æ­£æ–‡: {desc_len} å­—")
                print(f"    ğŸ’¬ è¯„è®º: {comment_count} æ¡")
                print(f"    ğŸ–¼ï¸ å›¾ç‰‡: {img_count} å¼ ")
                print(f"    ğŸ’¾ å·²ä¿å­˜: {filename}")
                
            else:
                print(f"    âŒ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                detailed_contents.append({
                    'note_id': note['id'],
                    'title': note['title'],
                    'fetch_success': False,
                    'error': result.get('message', 'æœªçŸ¥é”™è¯¯')
                })
            
            # é—´éš” 3 ç§’ï¼Œé¿å…é£æ§
            time.sleep(3)
            
        except Exception as e:
            print(f"    âŒ é”™è¯¯: {e}")
            detailed_contents.append({
                'note_id': note['id'],
                'title': note['title'],
                'fetch_success': False,
                'error': str(e)
            })
    
    # ä¿å­˜æ±‡æ€»
    summary = {
        'query': crawl_data.get('query', ''),
        'total_selected': len(high_quality),
        'success_count': sum(1 for d in detailed_contents if d.get('fetch_success', True)),
        'contents': detailed_contents,
        'saved_to': corpus_dir,
        'generated_at': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    summary_path = os.path.join(corpus_dir, 'summary_ai_contents.json')
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*70}")
    print(f"âœ… è¯¦æƒ…æŠ“å–å®Œæˆ")
    print(f"   æˆåŠŸ: {summary['success_count']}/{summary['total_selected']}")
    print(f"   ä¿å­˜ä½ç½®: {corpus_dir}")
    print(f"   æ±‡æ€»æ–‡ä»¶: summary_ai_contents.json")
    print(f"{'='*70}")
    
    return summary

if __name__ == "__main__":
    fetch_high_quality_details()
