#!/usr/bin/env python3
"""
Xiaohongshu Content Crawler
æŠ“å–å°çº¢ä¹¦æœç´¢ç»“æœçš„ç¬”è®°å†…å®¹
"""

import argparse
import json
import sys
import re
from datetime import datetime
from pathlib import Path

# Note: This is a template. Actual implementation requires:
# - playwright/selenium for browser automation
# - xhs-api or reverse-engineered API calls

def generate_crawl_template(topic: str, queries: list) -> str:
    """Generate a markdown template for crawled content"""
    date_str = datetime.now().strftime("%Y-%m-%d")
    
    content = f"""---
topic: {topic}
source_platform: xiaohongshu
query_list:
{chr(10).join(f'  - {q}' for q in queries)}
crawled_at: {date_str}
curated: false
---

# æŠ“å–ç»“æœ: {topic}

> æŠ“å–æ—¶é—´: {date_str}
> æœç´¢Query: {', '.join(queries)}

## å¾…æŠ“å–å†…å®¹åˆ—è¡¨

<!-- ä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ–å·¥å…·æŠ“å–ä»¥ä¸‹å†…å®¹ -->

### ç¬”è®°1
- **æ ‡é¢˜**: [å¾…å¡«å†™]
- **ä½œè€…**: [å¾…å¡«å†™]
- **é“¾æ¥**: [å¾…å¡«å†™]
- **ç‚¹èµæ•°**: [å¾…å¡«å†™]
- **æ”¶è—æ•°**: [å¾…å¡«å†™]
- **å†…å®¹æ‘˜è¦**: [å¾…å¡«å†™]
- **æ ‡ç­¾**: [å¾…å¡«å†™]
- **å›¾ç‰‡**: [å¾…å¡«å†™]

### ç¬”è®°2
...

---

## äººå·¥ç¡®è®¤åŒº

- [ ] å†…å®¹1å·²å®¡æ ¸ - è´¨é‡: [é«˜/ä¸­/ä½] - æ˜¯å¦å¯ç”¨: [æ˜¯/å¦]
- [ ] å†…å®¹2å·²å®¡æ ¸ - è´¨é‡: [é«˜/ä¸­/ä½] - æ˜¯å¦å¯ç”¨: [æ˜¯/å¦]

ç¡®è®¤åæ‰§è¡Œ: `python3 scripts/curate_content.py corpus/raw/{date_str}-{topic}.md`
"""
    return content

def expand_queries(topic: str) -> list:
    """æ ¹æ®ä¸»é¢˜æ‰©å±•æœç´¢è¯"""
    # åŸºç¡€æ‰©å±•è§„åˆ™
    expansions = {
        "ç©¿æ­": ["ç©¿æ­", "OOTD", "æ¯æ—¥ç©¿æ­", "æ­é…"],
        "ç¾å¦†": ["ç¾å¦†", "åŒ–å¦†", "æŠ¤è‚¤", "å½©å¦†"],
        "ç¾é£Ÿ": ["ç¾é£Ÿ", "æ¢åº—", "é£Ÿè°±", "æ–™ç†"],
        "æ—…è¡Œ": ["æ—…è¡Œ", "æ—…æ¸¸", "æ¸¸è®°", "æ”»ç•¥"],
        "å®¶å±…": ["å®¶å±…", "è£…ä¿®", "æ”¶çº³", "è½¯è£…"],
    }
    
    queries = [topic]
    for key, values in expansions.items():
        if key in topic:
            queries.extend(values)
    
    # å»é‡å¹¶ä¿æŒé¡ºåº
    seen = set()
    unique_queries = []
    for q in queries:
        if q not in seen:
            seen.add(q)
            unique_queries.append(q)
    
    return unique_queries[:5]  # æœ€å¤š5ä¸ªquery

def main():
    parser = argparse.ArgumentParser(description='Crawl Xiaohongshu content')
    parser.add_argument('topic', help='Topic to search for')
    parser.add_argument('--workspace', default='content-ops-workspace', 
                       help='Workspace directory')
    parser.add_argument('--queries', nargs='+', help='Custom search queries')
    
    args = parser.parse_args()
    
    # Expand or use custom queries
    queries = args.queries if args.queries else expand_queries(args.topic)
    
    # Generate filename
    date_str = datetime.now().strftime("%Y-%m-%d")
    safe_topic = re.sub(r'[^\w\s-]', '', args.topic).strip().replace(' ', '-')
    filename = f"{date_str}-{safe_topic}.md"
    
    # Ensure directory exists
    workspace = Path(args.workspace)
    raw_dir = workspace / "corpus" / "raw"
    raw_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate content
    content = generate_crawl_template(args.topic, queries)
    
    # Write file
    filepath = raw_dir / filename
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"âœ… æŠ“å–æ¨¡æ¿å·²åˆ›å»º: {filepath}")
    print(f"ğŸ“‹ ä¸»é¢˜: {args.topic}")
    print(f"ğŸ” æœç´¢Query: {', '.join(queries)}")
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"   1. ä½¿ç”¨æµè§ˆå™¨å·¥å…·æŠ“å–å†…å®¹å¹¶å¡«å…¥æ¨¡æ¿")
    print(f"   2. äººå·¥å®¡æ ¸å†…å®¹è´¨é‡")
    print(f"   3. ç¡®è®¤åæ‰§è¡Œ: python3 scripts/curate_content.py {filepath}")

if __name__ == '__main__':
    main()
